{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Transfer Learning\n",
    "\n",
    "Most of the time you won't want to train a whole convolutional network yourself. Modern ConvNets training on huge datasets like ImageNet take weeks on multiple GPUs. Instead, most people use a pretrained network either as a fixed feature extractor, or as an initial network to fine tune. In this notebook, you'll be using [VGGNet](https://arxiv.org/pdf/1409.1556.pdf) trained on the [ImageNet dataset](http://www.image-net.org/) as a feature extractor. Below is a diagram of the VGGNet architecture.\n",
    "\n",
    "<img src=\"assets/cnnarchitecture.jpg\" width=700px>\n",
    "\n",
    "VGGNet is great because it's simple and has great performance, coming in second in the ImageNet competition. The idea here is that we keep all the convolutional layers, but replace the final fully connected layers with our own classifier. This way we can use VGGNet as a feature extractor for our images then easily train a simple classifier on top of that. What we'll do is take the first fully connected layer with 4096 units, including thresholding with ReLUs. We can use those values as a code for each image, then build a classifier on top of those codes.\n",
    "\n",
    "You can read more about transfer learning from [the CS231n course notes](http://cs231n.github.io/transfer-learning/#tf).\n",
    "\n",
    "## Pretrained VGGNet\n",
    "\n",
    "We'll be using a pretrained network from https://github.com/machrisaa/tensorflow-vgg. Make sure to clone this repository to the directory you're working from. You'll also want to rename it so it has an underscore instead of a dash.\n",
    "\n",
    "```\n",
    "git clone https://github.com/machrisaa/tensorflow-vgg.git tensorflow_vgg\n",
    "```\n",
    "\n",
    "This is a really nice implementation of VGGNet, quite easy to work with. The network has already been trained and the parameters are available from this link. **You'll need to clone the repo into the folder containing this notebook.** Then download the parameter file using the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter file already exists!\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "\n",
    "vgg_dir = 'tensorflow_vgg/'\n",
    "# Make sure vgg exists\n",
    "if not isdir(vgg_dir):\n",
    "    raise Exception(\"VGG directory doesn't exist!\")\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(vgg_dir + \"vgg16.npy\"):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='VGG16 Parameters') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://s3.amazonaws.com/content.udacity-data.com/nd101/vgg16.npy',\n",
    "            vgg_dir + 'vgg16.npy',\n",
    "            pbar.hook)\n",
    "else:\n",
    "    print(\"Parameter file already exists!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Flower power\n",
    "\n",
    "Here we'll be using VGGNet to classify images of flowers. To get the flower dataset, run the cell below. This dataset comes from the [TensorFlow inception tutorial](https://www.tensorflow.org/tutorials/image_retraining)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flowers Dataset: 229MB [00:24, 9.49MB/s]                              \n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "dataset_folder_path = 'flower_photos'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('flower_photos.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='Flowers Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'http://download.tensorflow.org/example_images/flower_photos.tgz',\n",
    "            'flower_photos.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(dataset_folder_path):\n",
    "    with tarfile.open('flower_photos.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## ConvNet Codes\n",
    "\n",
    "Below, we'll run through all the images in our dataset and get codes for each of them. That is, we'll run the images through the VGGNet convolutional layers and record the values of the first fully connected layer. We can then write these to a file for later when we build our own classifier.\n",
    "\n",
    "Here we're using the `vgg16` module from `tensorflow_vgg`. The network takes images of size $224 \\times 224 \\times 3$ as input. Then it has 5 sets of convolutional layers. The network implemented here has this structure (copied from [the source code](https://github.com/machrisaa/tensorflow-vgg/blob/master/vgg16.py)):\n",
    "\n",
    "```\n",
    "self.conv1_1 = self.conv_layer(bgr, \"conv1_1\")\n",
    "self.conv1_2 = self.conv_layer(self.conv1_1, \"conv1_2\")\n",
    "self.pool1 = self.max_pool(self.conv1_2, 'pool1')\n",
    "\n",
    "self.conv2_1 = self.conv_layer(self.pool1, \"conv2_1\")\n",
    "self.conv2_2 = self.conv_layer(self.conv2_1, \"conv2_2\")\n",
    "self.pool2 = self.max_pool(self.conv2_2, 'pool2')\n",
    "\n",
    "self.conv3_1 = self.conv_layer(self.pool2, \"conv3_1\")\n",
    "self.conv3_2 = self.conv_layer(self.conv3_1, \"conv3_2\")\n",
    "self.conv3_3 = self.conv_layer(self.conv3_2, \"conv3_3\")\n",
    "self.pool3 = self.max_pool(self.conv3_3, 'pool3')\n",
    "\n",
    "self.conv4_1 = self.conv_layer(self.pool3, \"conv4_1\")\n",
    "self.conv4_2 = self.conv_layer(self.conv4_1, \"conv4_2\")\n",
    "self.conv4_3 = self.conv_layer(self.conv4_2, \"conv4_3\")\n",
    "self.pool4 = self.max_pool(self.conv4_3, 'pool4')\n",
    "\n",
    "self.conv5_1 = self.conv_layer(self.pool4, \"conv5_1\")\n",
    "self.conv5_2 = self.conv_layer(self.conv5_1, \"conv5_2\")\n",
    "self.conv5_3 = self.conv_layer(self.conv5_2, \"conv5_3\")\n",
    "self.pool5 = self.max_pool(self.conv5_3, 'pool5')\n",
    "\n",
    "self.fc6 = self.fc_layer(self.pool5, \"fc6\")\n",
    "self.relu6 = tf.nn.relu(self.fc6)\n",
    "```\n",
    "\n",
    "So what we want are the values of the first fully connected layer, after being ReLUd (`self.relu6`). To build the network, we use\n",
    "\n",
    "```\n",
    "with tf.Session() as sess:\n",
    "    vgg = vgg16.Vgg16()\n",
    "    input_ = tf.placeholder(tf.float32, [None, 224, 224, 3])\n",
    "    with tf.name_scope(\"content_vgg\"):\n",
    "        vgg.build(input_)\n",
    "```\n",
    "\n",
    "This creates the `vgg` object, then builds the graph with `vgg.build(input_)`. Then to get the values from the layer,\n",
    "\n",
    "```\n",
    "feed_dict = {input_: images}\n",
    "codes = sess.run(vgg.relu6, feed_dict=feed_dict)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow_vgg import vgg16\n",
    "from tensorflow_vgg import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_dir = 'flower_photos/'\n",
    "contents = os.listdir(data_dir)\n",
    "classes = [each for each in contents if os.path.isdir(data_dir + each)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Below I'm running images through the VGG network in batches.\n",
    "\n",
    "> **Exercise:** Below, build the VGG network. Also get the codes from the first fully connected layer (make sure you get the ReLUd values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/aravinds/Dropbox/MIDS/Deeplearning/deep-learning/transfer-learning/tensorflow_vgg/vgg16.npy\n",
      "npy file loaded\n",
      "build model started\n",
      "build model finished: 0s\n",
      "Starting daisy images\n",
      "10 images processed\n",
      "20 images processed\n",
      "30 images processed\n",
      "40 images processed\n",
      "50 images processed\n",
      "60 images processed\n",
      "70 images processed\n",
      "80 images processed\n",
      "90 images processed\n",
      "100 images processed\n",
      "110 images processed\n",
      "120 images processed\n",
      "130 images processed\n",
      "140 images processed\n",
      "150 images processed\n",
      "160 images processed\n",
      "170 images processed\n",
      "180 images processed\n",
      "190 images processed\n",
      "200 images processed\n",
      "210 images processed\n",
      "220 images processed\n",
      "230 images processed\n",
      "240 images processed\n",
      "250 images processed\n",
      "260 images processed\n",
      "270 images processed\n",
      "280 images processed\n",
      "290 images processed\n",
      "300 images processed\n",
      "310 images processed\n",
      "320 images processed\n",
      "330 images processed\n",
      "340 images processed\n",
      "350 images processed\n",
      "360 images processed\n",
      "370 images processed\n",
      "380 images processed\n",
      "390 images processed\n",
      "400 images processed\n",
      "410 images processed\n",
      "420 images processed\n",
      "430 images processed\n",
      "440 images processed\n",
      "450 images processed\n",
      "460 images processed\n",
      "470 images processed\n",
      "480 images processed\n",
      "490 images processed\n",
      "500 images processed\n",
      "510 images processed\n",
      "520 images processed\n",
      "530 images processed\n",
      "540 images processed\n",
      "550 images processed\n",
      "560 images processed\n",
      "570 images processed\n",
      "580 images processed\n",
      "590 images processed\n",
      "600 images processed\n",
      "610 images processed\n",
      "620 images processed\n",
      "630 images processed\n",
      "633 images processed\n",
      "Starting dandelion images\n",
      "10 images processed\n",
      "20 images processed\n",
      "30 images processed\n",
      "40 images processed\n",
      "50 images processed\n",
      "60 images processed\n",
      "70 images processed\n",
      "80 images processed\n",
      "90 images processed\n",
      "100 images processed\n",
      "110 images processed\n",
      "120 images processed\n",
      "130 images processed\n",
      "140 images processed\n",
      "150 images processed\n",
      "160 images processed\n",
      "170 images processed\n",
      "180 images processed\n",
      "190 images processed\n",
      "200 images processed\n",
      "210 images processed\n",
      "220 images processed\n",
      "230 images processed\n",
      "240 images processed\n",
      "250 images processed\n",
      "260 images processed\n",
      "270 images processed\n",
      "280 images processed\n",
      "290 images processed\n",
      "300 images processed\n",
      "310 images processed\n",
      "320 images processed\n",
      "330 images processed\n",
      "340 images processed\n",
      "350 images processed\n",
      "360 images processed\n",
      "370 images processed\n",
      "380 images processed\n",
      "390 images processed\n",
      "400 images processed\n",
      "410 images processed\n",
      "420 images processed\n",
      "430 images processed\n",
      "440 images processed\n",
      "450 images processed\n",
      "460 images processed\n",
      "470 images processed\n",
      "480 images processed\n",
      "490 images processed\n",
      "500 images processed\n",
      "510 images processed\n",
      "520 images processed\n",
      "530 images processed\n",
      "540 images processed\n",
      "550 images processed\n",
      "560 images processed\n",
      "570 images processed\n",
      "580 images processed\n",
      "590 images processed\n",
      "600 images processed\n",
      "610 images processed\n",
      "620 images processed\n",
      "630 images processed\n",
      "640 images processed\n",
      "650 images processed\n",
      "660 images processed\n",
      "670 images processed\n",
      "680 images processed\n",
      "690 images processed\n",
      "700 images processed\n",
      "710 images processed\n",
      "720 images processed\n",
      "730 images processed\n",
      "740 images processed\n",
      "750 images processed\n",
      "760 images processed\n",
      "770 images processed\n",
      "780 images processed\n",
      "790 images processed\n",
      "800 images processed\n",
      "810 images processed\n",
      "820 images processed\n",
      "830 images processed\n",
      "840 images processed\n",
      "850 images processed\n",
      "860 images processed\n",
      "870 images processed\n",
      "880 images processed\n",
      "890 images processed\n",
      "898 images processed\n",
      "Starting roses images\n",
      "10 images processed\n",
      "20 images processed\n",
      "30 images processed\n",
      "40 images processed\n",
      "50 images processed\n",
      "60 images processed\n",
      "70 images processed\n",
      "80 images processed\n",
      "90 images processed\n",
      "100 images processed\n",
      "110 images processed\n",
      "120 images processed\n",
      "130 images processed\n",
      "140 images processed\n",
      "150 images processed\n",
      "160 images processed\n",
      "170 images processed\n",
      "180 images processed\n",
      "190 images processed\n",
      "200 images processed\n",
      "210 images processed\n",
      "220 images processed\n",
      "230 images processed\n",
      "240 images processed\n",
      "250 images processed\n",
      "260 images processed\n",
      "270 images processed\n",
      "280 images processed\n",
      "290 images processed\n",
      "300 images processed\n",
      "310 images processed\n",
      "320 images processed\n",
      "330 images processed\n",
      "340 images processed\n",
      "350 images processed\n",
      "360 images processed\n",
      "370 images processed\n",
      "380 images processed\n",
      "390 images processed\n",
      "400 images processed\n",
      "410 images processed\n",
      "420 images processed\n",
      "430 images processed\n",
      "440 images processed\n",
      "450 images processed\n",
      "460 images processed\n",
      "470 images processed\n",
      "480 images processed\n",
      "490 images processed\n",
      "500 images processed\n",
      "510 images processed\n",
      "520 images processed\n",
      "530 images processed\n",
      "540 images processed\n",
      "550 images processed\n",
      "560 images processed\n",
      "570 images processed\n",
      "580 images processed\n",
      "590 images processed\n",
      "600 images processed\n",
      "610 images processed\n",
      "620 images processed\n",
      "630 images processed\n",
      "640 images processed\n",
      "641 images processed\n",
      "Starting sunflowers images\n",
      "10 images processed\n",
      "20 images processed\n",
      "30 images processed\n",
      "40 images processed\n",
      "50 images processed\n",
      "60 images processed\n",
      "70 images processed\n",
      "80 images processed\n",
      "90 images processed\n",
      "100 images processed\n",
      "110 images processed\n",
      "120 images processed\n",
      "130 images processed\n",
      "140 images processed\n",
      "150 images processed\n",
      "160 images processed\n",
      "170 images processed\n",
      "180 images processed\n",
      "190 images processed\n",
      "200 images processed\n",
      "210 images processed\n",
      "220 images processed\n",
      "230 images processed\n",
      "240 images processed\n",
      "250 images processed\n",
      "260 images processed\n",
      "270 images processed\n",
      "280 images processed\n",
      "290 images processed\n",
      "300 images processed\n",
      "310 images processed\n",
      "320 images processed\n",
      "330 images processed\n",
      "340 images processed\n",
      "350 images processed\n",
      "360 images processed\n",
      "370 images processed\n",
      "380 images processed\n",
      "390 images processed\n",
      "400 images processed\n",
      "410 images processed\n",
      "420 images processed\n",
      "430 images processed\n",
      "440 images processed\n",
      "450 images processed\n",
      "460 images processed\n",
      "470 images processed\n",
      "480 images processed\n",
      "490 images processed\n",
      "500 images processed\n",
      "510 images processed\n",
      "520 images processed\n",
      "530 images processed\n",
      "540 images processed\n",
      "550 images processed\n",
      "560 images processed\n",
      "570 images processed\n",
      "580 images processed\n",
      "590 images processed\n",
      "600 images processed\n",
      "610 images processed\n",
      "620 images processed\n",
      "630 images processed\n",
      "640 images processed\n",
      "650 images processed\n",
      "660 images processed\n",
      "670 images processed\n",
      "680 images processed\n",
      "690 images processed\n",
      "699 images processed\n",
      "Starting tulips images\n",
      "10 images processed\n",
      "20 images processed\n",
      "30 images processed\n",
      "40 images processed\n",
      "50 images processed\n",
      "60 images processed\n",
      "70 images processed\n",
      "80 images processed\n",
      "90 images processed\n",
      "100 images processed\n",
      "110 images processed\n",
      "120 images processed\n",
      "130 images processed\n",
      "140 images processed\n",
      "150 images processed\n",
      "160 images processed\n",
      "170 images processed\n",
      "180 images processed\n",
      "190 images processed\n",
      "200 images processed\n",
      "210 images processed\n",
      "220 images processed\n",
      "230 images processed\n",
      "240 images processed\n",
      "250 images processed\n",
      "260 images processed\n",
      "270 images processed\n",
      "280 images processed\n",
      "290 images processed\n",
      "300 images processed\n",
      "310 images processed\n",
      "320 images processed\n",
      "330 images processed\n",
      "340 images processed\n",
      "350 images processed\n",
      "360 images processed\n",
      "370 images processed\n",
      "380 images processed\n",
      "390 images processed\n",
      "400 images processed\n",
      "410 images processed\n",
      "420 images processed\n",
      "430 images processed\n",
      "440 images processed\n",
      "450 images processed\n",
      "460 images processed\n",
      "470 images processed\n",
      "480 images processed\n",
      "490 images processed\n",
      "500 images processed\n",
      "510 images processed\n",
      "520 images processed\n",
      "530 images processed\n",
      "540 images processed\n",
      "550 images processed\n",
      "560 images processed\n",
      "570 images processed\n",
      "580 images processed\n",
      "590 images processed\n",
      "600 images processed\n",
      "610 images processed\n",
      "620 images processed\n",
      "630 images processed\n",
      "640 images processed\n",
      "650 images processed\n",
      "660 images processed\n",
      "670 images processed\n",
      "680 images processed\n",
      "690 images processed\n",
      "700 images processed\n",
      "710 images processed\n",
      "720 images processed\n",
      "730 images processed\n",
      "740 images processed\n",
      "750 images processed\n",
      "760 images processed\n",
      "770 images processed\n",
      "780 images processed\n",
      "790 images processed\n",
      "799 images processed\n"
     ]
    }
   ],
   "source": [
    "# Set the batch size higher if you can fit in in your GPU memory\n",
    "batch_size = 10\n",
    "codes_list = []\n",
    "labels = []\n",
    "batch = []\n",
    "\n",
    "codes = None\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    vgg = vgg16.Vgg16()\n",
    "    input_ = tf.placeholder(tf.float32, [None,224,224,3])\n",
    "    with tf.name_scope(\"content_vgg\"):\n",
    "        vgg.build(input_)\n",
    "\n",
    "    for each in classes:\n",
    "        print(\"Starting {} images\".format(each))\n",
    "        class_path = data_dir + each\n",
    "        files = os.listdir(class_path)\n",
    "        for ii, file in enumerate(files, 1):\n",
    "            # Add images to the current batch\n",
    "            # utils.load_image crops the input images for us, from the center\n",
    "            img = utils.load_image(os.path.join(class_path, file))\n",
    "            batch.append(img.reshape((1, 224, 224, 3)))\n",
    "            labels.append(each)\n",
    "            \n",
    "            # Running the batch through the network to get the codes\n",
    "            if ii % batch_size == 0 or ii == len(files):\n",
    "                \n",
    "                # Image batch to pass to VGG network\n",
    "                images = np.concatenate(batch)\n",
    "                \n",
    "                # TODO: Get the values from the relu6 layer of the VGG network\n",
    "                feed_dict = {input_: images}\n",
    "                codes_batch = sess.run(vgg.relu6, feed_dict=feed_dict)\n",
    "                \n",
    "                # Here I'm building an array of the codes\n",
    "                if codes is None:\n",
    "                    codes = codes_batch\n",
    "                else:\n",
    "                    codes = np.concatenate((codes, codes_batch))\n",
    "                \n",
    "                # Reset to start building the next batch\n",
    "                batch = []\n",
    "                print('{} images processed'.format(ii))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3670, 4096)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write codes to file\n",
    "with open('codes', 'w') as f:\n",
    "    codes.tofile(f)\n",
    "    \n",
    "# write labels to file\n",
    "import csv\n",
    "with open('labels', 'w') as f:\n",
    "    writer = csv.writer(f, delimiter='\\n')\n",
    "    writer.writerow(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Building the Classifier\n",
    "\n",
    "Now that we have codes for all the images, we can build a simple classifier on top of them. The codes behave just like normal input into a simple neural network. Below I'm going to have you do most of the work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read codes and labels from file\n",
    "import csv\n",
    "\n",
    "with open('labels') as f:\n",
    "    reader = csv.reader(f, delimiter='\\n')\n",
    "    labels = np.array([each for each in reader if len(each) > 0]).squeeze()\n",
    "with open('codes') as f:\n",
    "    codes = np.fromfile(f, dtype=np.float32)\n",
    "    codes = codes.reshape((len(labels), -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Data prep\n",
    "\n",
    "As usual, now we need to one-hot encode our labels and create validation/test sets. First up, creating our labels!\n",
    "\n",
    "> **Exercise:** From scikit-learn, use [LabelBinarizer](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html) to create one-hot encoded vectors from the labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "lb = LabelBinarizer()\n",
    "lb_x = lb.fit(labels)\n",
    "\n",
    "\n",
    "labels_vecs = lb.transform(labels)# Your one-hot encoded labels array here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now you'll want to create your training, validation, and test sets. An important thing to note here is that our labels and data aren't randomized yet. We'll want to shuffle our data so the validation and test sets contain data from all classes. Otherwise, you could end up with testing sets that are all one class. Typically, you'll also want to make sure that each smaller set has the same the distribution of classes as it is for the whole data set. The easiest way to accomplish both these goals is to use [`StratifiedShuffleSplit`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html) from scikit-learn.\n",
    "\n",
    "You can create the splitter like so:\n",
    "```\n",
    "ss = StratifiedShuffleSplit(n_splits=1, test_size=0.2)\n",
    "```\n",
    "Then split the data with \n",
    "```\n",
    "splitter = ss.split(x, y)\n",
    "```\n",
    "\n",
    "`ss.split` returns a generator of indices. You can pass the indices into the arrays to get the split sets. The fact that it's a generator means you either need to iterate over it, or use `next(splitter)` to get the indices. Be sure to read the [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html) and the [user guide](http://scikit-learn.org/stable/modules/cross_validation.html#random-permutations-cross-validation-a-k-a-shuffle-split).\n",
    "\n",
    "> **Exercise:** Use StratifiedShuffleSplit to split the codes and labels into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2604 2084 2240 ...,   86 2501  834]\n",
      "[2472 1058 2674 3504 1599 1073  633 1565  430 2474 2495 1385  246 1842 1068\n",
      " 3085 1856 2638 2315 2972 1176 1721 3130 3052 3286 1994  848 1618 1336 1093\n",
      " 2850 1357 3471  456  321   22 1494  939 3268  797  954 2612  588  571 1918\n",
      "  111 1288 3153 1438  470 1443 3385 3102 2610 1010 2525 3647 2344 3363 3589\n",
      "  784 2651 2731  929 2370 2702 1560  836 2442  973  883  892 1820 2210 1597\n",
      " 3003 3359  978 3617 1480 2895  107  464  241 3434 2213 2085 3150 2138 2803\n",
      "  976 2560  175  281 2127 1789 3338  661   16  591  683 1793 2037 1751  356\n",
      " 1991 3401 1574 1391 1422 3117 1310  408  828 1522 3332  754 2369 2504 2091\n",
      " 1775 2125 1727  305 1323 1852  451 1750  288 1763 1681  419 1395 2140  807\n",
      " 1075 1940 1504 1468 1687 1102 1154 1951  342 2267 3173 3483 3262 1476 3210\n",
      "  899  436 1394 1065 2824 2330 1389 1198 2270 2077 3318  988 2880 2570 1363\n",
      " 2265 3569 2540 2284 2600 3118 1957  346 3465  680 3065 1083 1554 2931 2023\n",
      "  165 1298 1890  873  903 2924 2847 1959  534 1469 2552  681  733 1051 1257\n",
      " 2361 3139 1441 1081 2108  414  179 1985  693  422 2047  637 2092  268 1737\n",
      " 2768 3250 1586   91  919  545 2710 3602  563 2701 2593 1270  265 2397 2298\n",
      " 2885  585 2984 3621 3219 1200 3559 3460 2798 1138 2312  617 2206  935 1003\n",
      " 3556  343 1657   44  245 2695 1103 3588 2436 3265  761 3221 2661 3040 2714\n",
      "  647  413 3086 1739  311  751 1703  110 1613 3115 2446 1646 1806  535 1467\n",
      " 2380 2997 1343 2109  136 1986  261 2579 3653 1150  380  882 2031 3230  947\n",
      " 3417 2994 3343 2054  431 2239 3121 1246 3014  266  748 3596 1419   73  694\n",
      "  238 1225 3143 2082 1477 2216 1713 3657 2897 3109  731  405 2753 1724 2801\n",
      "  729 2059  234 3391 2720 2490  177 2366 3587 1843 2410  370 2457 2110  230\n",
      " 3149 2234  282 2561  276 1099 2999 3253 1819  287 2948 3450 1784 3034 3266\n",
      " 1732 3478 2505 3026 3043 2214 3164 3387  608 1873 1505 1372 1742 2362 2677\n",
      " 1669 1420 2427 2482  479 3005 1814 2191 3092  454 2962 3466  476 1767  438\n",
      " 3025 1140  327 3206  404  354 1702 1384 2990  205 1080 1158 1530 1960 1512\n",
      " 3334  541 1580 2696  248 1997 1421 1479  411  667 2745 2162 1802  572 2518\n",
      " 2155 2401 3423  738 1913  983 1518 2017 2891 2920 1865 3284  551 3004 3070\n",
      " 3426 3428 1006 2246 3650  164 1810 2817 2634 3327 3322 1662  599 3516 1206\n",
      "  200 3353 2055 1744  279  864  195  814 1803 3410 3281 1834 1187 1107  514\n",
      "   52  387 1868 2538 1655  423  157 2203 1894 2894 2794 2676 3454  790  153\n",
      " 2262 3114  851 2609 2919  887  994  849  532 1488 3198  969  949  698 1859\n",
      " 1576 1824 2955 3641 3228  323 1564   31 3366  789 2660 2635  846 1531 1626\n",
      " 3063  881 1464 2488  912 1393  686 2205 2296 2793 1975  614 3277 1128 2535\n",
      "   42 1018 2512  583  996  842  485 1362 1417 1771  948 2351 3216  656 1039\n",
      " 2447 1923   85  783  709 1840 1787  338 3435  764 2255 2428 3665 2449 2515\n",
      " 3129 2700 1860 1234 3584 1910  272 3510 3490   53 2694 3597 2556 3652  355\n",
      " 1284  906  603 2659 2227 2749 3016 2991  832 3075 2111  503 2185  914  895\n",
      " 1286 3476  167 3542 3329 1532  461 2568   62 2253 2314 2742 1038 2937 2805\n",
      " 1032 1399 1300 3627 3464  135 2167 2280  625  220  579  262  735 1322 1223\n",
      " 2451 1668 1181 3200 3507  176 3055 2317 2308 1235 2389 3009 2935 2425  352\n",
      " 3622  925 1656  455 2625 1321    0 2870 1619 1379 2732 2078 1909 1559 1664\n",
      " 2608 2983 2641 2649 1776 1778  151 2527 2043 2118 2481 2417  970 2903 3379\n",
      "  992  876 1871  449 1801 2688 1708  428 3051 3234 3386 1122 1607 1829 3563\n",
      " 2326   74  962 3614  161 2058 1745 2849 3316 2782  623 1589 2892 1196 1060\n",
      " 2750 2316 3050   55 2135 2703 2208 1213  460 3083 1942 2631 3089 1021 2455\n",
      " 2648 1210 1817 3566 1281   10 1171  674  960 2709 1036 2221  924 3314 3381\n",
      " 2120  172 3562 3057  819 3424 1162 1396 1544  500 1388 1218 1203 2573 3225\n",
      " 2279   34 3437 1935  523 1334 2507 2211  444 3664  432 2419  389 2907]\n"
     ]
    }
   ],
   "source": [
    "ss = StratifiedShuffleSplit(n_splits=1, test_size=0.2)\n",
    "splitter1 = ss.split(codes, labels)\n",
    "\n",
    "for i,j in splitter1:\n",
    "    print(i)\n",
    "    print(j)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'generator' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-8959fced8131>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplitter1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplitter1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplitter1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplitter1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mss2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStratifiedShuffleSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msplitter2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mss2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'generator' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "\n",
    "train_x, train_y = codes[splitter1[0]], labels[splitter1[0]]\n",
    "val_x, val_y = codes[splitter1[1]], labels[splitter1[1]]\n",
    "ss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.5)\n",
    "splitter2 = ss2.split(val_x, val_y)\n",
    "test_x, test_y =  val_x[splitter2[1]], val_y[splitter2[1]]\n",
    "val_x, val_y = val_x[splitter2[0]], val_y[splitter2[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"Train shapes (x, y):\", train_x.shape, train_y.shape)\n",
    "print(\"Validation shapes (x, y):\", val_x.shape, val_y.shape)\n",
    "print(\"Test shapes (x, y):\", test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "If you did it right, you should see these sizes for the training sets:\n",
    "\n",
    "```\n",
    "Train shapes (x, y): (2936, 4096) (2936, 5)\n",
    "Validation shapes (x, y): (367, 4096) (367, 5)\n",
    "Test shapes (x, y): (367, 4096) (367, 5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Classifier layers\n",
    "\n",
    "Once you have the convolutional codes, you just need to build a classfier from some fully connected layers. You use the codes as the inputs and the image labels as targets. Otherwise the classifier is a typical neural network.\n",
    "\n",
    "> **Exercise:** With the codes and labels loaded, build the classifier. Consider the codes as your inputs, each of them are 4096D vectors. You'll want to use a hidden layer and an output layer as your classifier. Remember that the output layer needs to have one unit for each class and a softmax activation function. Use the cross entropy to calculate the cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "inputs_ = tf.placeholder(tf.float32, shape=[None, codes.shape[1]])\n",
    "labels_ = tf.placeholder(tf.int64, shape=[None, labels_vecs.shape[1]])\n",
    "\n",
    "# TODO: Classifier layers and operations\n",
    "\n",
    "logits = # output layer logits\n",
    "cost = # cross entropy loss\n",
    "\n",
    "optimizer = # training optimizer\n",
    "\n",
    "# Operations for validation/test accuracy\n",
    "predicted = tf.nn.softmax(logits)\n",
    "correct_pred = tf.equal(tf.argmax(predicted, 1), tf.argmax(labels_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Batches!\n",
    "\n",
    "Here is just a simple way to do batches. I've written it so that it includes all the data. Sometimes you'll throw out some data at the end to make sure you have full batches. Here I just extend the last batch to include the remaining data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_batches(x, y, n_batches=10):\n",
    "    \"\"\" Return a generator that yields batches from arrays x and y. \"\"\"\n",
    "    batch_size = len(x)//n_batches\n",
    "    \n",
    "    for ii in range(0, n_batches*batch_size, batch_size):\n",
    "        # If we're not on the last batch, grab data with size batch_size\n",
    "        if ii != (n_batches-1)*batch_size:\n",
    "            X, Y = x[ii: ii+batch_size], y[ii: ii+batch_size] \n",
    "        # On the last batch, grab the rest of the data\n",
    "        else:\n",
    "            X, Y = x[ii:], y[ii:]\n",
    "        # I love generators\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Training\n",
    "\n",
    "Here, we'll train the network.\n",
    "\n",
    "> **Exercise:** So far we've been providing the training code for you. Here, I'm going to give you a bit more of a challenge and have you write the code to train the network. Of course, you'll be able to see my solution if you need help. Use the `get_batches` function I wrote before to get your batches like `for x, y in get_batches(train_x, train_y)`. Or write your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # TODO: Your training code here\n",
    "    saver.save(sess, \"checkpoints/flowers.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Testing\n",
    "\n",
    "Below you see the test accuracy. You can also see the predictions returned for images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    feed = {inputs_: test_x,\n",
    "            labels_: test_y}\n",
    "    test_acc = sess.run(accuracy, feed_dict=feed)\n",
    "    print(\"Test accuracy: {:.4f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import imread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Below, feel free to choose images and see how the trained classifier predicts the flowers in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_img_path = 'flower_photos/roses/10894627425_ec76bbc757_n.jpg'\n",
    "test_img = imread(test_img_path)\n",
    "plt.imshow(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Run this cell if you don't have a vgg graph built\n",
    "if 'vgg' in globals():\n",
    "    print('\"vgg\" object already exists.  Will not create again.')\n",
    "else:\n",
    "    #create vgg\n",
    "    with tf.Session() as sess:\n",
    "        input_ = tf.placeholder(tf.float32, [None, 224, 224, 3])\n",
    "        vgg = vgg16.Vgg16()\n",
    "        vgg.build(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    img = utils.load_image(test_img_path)\n",
    "    img = img.reshape((1, 224, 224, 3))\n",
    "\n",
    "    feed_dict = {input_: img}\n",
    "    code = sess.run(vgg.relu6, feed_dict=feed_dict)\n",
    "        \n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    feed = {inputs_: code}\n",
    "    prediction = sess.run(predicted, feed_dict=feed).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.barh(np.arange(5), prediction)\n",
    "_ = plt.yticks(np.arange(5), lb.classes_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
